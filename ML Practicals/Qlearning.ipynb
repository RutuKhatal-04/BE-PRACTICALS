{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "[[[ 4.20039616  3.08572369  4.17092496  4.845851  ]\n",
      "  [ 4.69778982  4.78138718  3.95270031  5.49539   ]\n",
      "  [ 5.37941298  6.2171      4.70804609  5.31170782]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.07175904 -0.04390937 -0.08410269 -0.06593549]]\n",
      "\n",
      " [[ 0.64688468  4.55416453  0.59882012  1.03532462]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 5.46014339  7.019       5.96615107  6.18286325]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.07365385  2.33158083 -0.08785025  0.21729709]]\n",
      "\n",
      " [[ 0.82808929  1.23183802  1.65693154  5.9092048 ]\n",
      "  [ 3.33913415  1.51737285  1.88361154  7.01109925]\n",
      "  [ 6.12613804  6.94140551  6.05658201  7.91      ]\n",
      "  [ 7.86909925  8.9         6.9441895   6.6153003 ]\n",
      "  [ 0.37006488  2.16797956  7.83114352  2.83720338]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 0.72143871  3.27339778  0.35479833  0.14015308]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 7.81454546 10.          8.60632849  8.68775734]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.029701   -0.0385948   0.02903185  0.95394823]\n",
      "  [ 0.30411323  0.7595236   0.06659166  6.04671666]\n",
      "  [ 2.56904364  2.86793637  0.93165751  8.90581011]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Maze layout represented as a 2D NumPy array\n",
    "# 0 = Free space, -1 = Obstacle, 1 = Goal (target for the agent)\n",
    "maze = np.array([\n",
    "    [0, 0, 0, -1, 0],\n",
    "    [0, -1, 0, -1, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [-1, 0, -1, 0, -1],\n",
    "    [0, 0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "# Hyperparameters for Q-learning\n",
    "alpha = 0.1  # Learning rate (controls how much new information overrides old information)\n",
    "gamma = 0.9  # Discount factor (weights the importance of future rewards)\n",
    "epsilon = 0.8  # Exploration rate (probability of taking a random action)\n",
    "epsilon_decay = 0.995  # Decay rate for epsilon (reduces exploration over time)\n",
    "num_episodes = 500  # Number of training episodes (iterations)\n",
    "\n",
    "# Initialize the Q-table with zeros\n",
    "# Q-table dimensions: (rows, columns, number of actions)\n",
    "# Each state (position) has a value for each action (up, down, left, right)\n",
    "q_table = np.zeros((maze.shape[0], maze.shape[1], 4))\n",
    "\n",
    "# Define possible actions with their corresponding changes in position\n",
    "# 0: Up (-1 in row), 1: Down (+1 in row), 2: Left (-1 in column), 3: Right (+1 in column)\n",
    "actions = {\n",
    "    0: (-1, 0),  # Move up\n",
    "    1: (1, 0),   # Move down\n",
    "    2: (0, -1),  # Move left\n",
    "    3: (0, 1)    # Move right\n",
    "}\n",
    "\n",
    "# Function to check if a position is valid (within bounds and not an obstacle)\n",
    "def is_valid_position(position):\n",
    "    x, y = position  # Unpack the coordinates\n",
    "    # Check if position is within the maze boundaries and not an obstacle\n",
    "    return 0 <= x < maze.shape[0] and 0 <= y < maze.shape[1] and maze[x, y] != -1\n",
    "\n",
    "# Function to get the next valid position based on the current position and chosen action\n",
    "def get_next_position(position, action):\n",
    "    x, y = position  # Current position\n",
    "    dx, dy = actions[action]  # Get changes in coordinates for the action\n",
    "    next_position = (x + dx, y + dy)  # Calculate the new position\n",
    "    # Return the new position if valid, otherwise stay in the current position\n",
    "    return next_position if is_valid_position(next_position) else position\n",
    "\n",
    "# Function to get the reward for the current position\n",
    "def get_reward(position):\n",
    "    # If the position is the goal, return a high reward\n",
    "    if maze[position] == 1:\n",
    "        return 10\n",
    "    # If the position is an obstacle, return a penalty\n",
    "    elif maze[position] == -1:\n",
    "        return -1\n",
    "    # Otherwise, return a small step penalty for moving\n",
    "    return -0.1\n",
    "\n",
    "# Main Q-learning loop for training the agent\n",
    "for episode in range(num_episodes):\n",
    "    position = (0, 0)  # Start at the top-left corner of the maze\n",
    "    done = False  # Flag to indicate if the episode is finished\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy policy: choose to explore or exploit\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, 3)  # Explore: select a random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[position[0], position[1]])  # Exploit: select the action with the highest Q-value\n",
    "\n",
    "        # Get the next position and the reward for that action\n",
    "        next_position = get_next_position(position, action)\n",
    "        reward = get_reward(next_position)\n",
    "\n",
    "        # Get the best action for the next state based on the Q-table\n",
    "        best_next_action = np.argmax(q_table[next_position[0], next_position[1]])\n",
    "\n",
    "        # Update Q-value using the Bellman equation:\n",
    "        # Q(state, action) = Q(state, action) + alpha * (reward + gamma * max(Q(next_state, next_action)) - Q(state, action))\n",
    "        q_table[position[0], position[1], action] += alpha * (\n",
    "            reward + gamma * q_table[next_position[0], next_position[1], best_next_action] -\n",
    "            q_table[position[0], position[1], action]\n",
    "        )\n",
    "\n",
    "        # Move to the next position\n",
    "        position = next_position\n",
    "\n",
    "        # Check if the goal is reached; end the episode if so\n",
    "        if maze[position] == 1:\n",
    "            done = True\n",
    "\n",
    "    # Decay the epsilon value after each episode to reduce the exploration rate\n",
    "    epsilon *= epsilon_decay\n",
    "\n",
    "# Print the Q-table after training is completed\n",
    "print(\"Training completed!\")\n",
    "print(q_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
