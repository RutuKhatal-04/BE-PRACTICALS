
import numpy as np
import random

#==================================Define the maze layout===========================================
maze = np.array([
    [0, 0, 0, -1, 0],
    [0, -1, 0, -1, 0],
    [0, 0, 0, 0, 0],
    [-1, 0, -1, 0, -1],
    [0, 0, 0, 1, 0]
])




#=================================Define hyperparameters=============================================
alpha = 0.1               # Learning rate
gamma = 0.9               # Discount factor
epsilon = 0.8             # Exploration rate
epsilon_decay = 0.995
num_episodes = 500





#==============================Initialize Q-table with zeros (size of the maze x number of actions)=========================================

q_table = np.zeros((maze.shape[0], maze.shape[1], 4)) # 4 possible actions (up, down, left, right)






#===========================================Define actions=======================================================================
actions = {
    0: (-1, 0),  # Up
    1: (1, 0),   # Down
    2: (0, -1),  # Left
    3: (0, 1)    # Right
}





#========================================Define function to check if a position is valid========================================

def is_valid_position(maze, position):
    x, y = position
    return 0 <= x < maze.shape[0] and 0 <= y < maze.shape[1] and maze[x, y] != -1





#=======================================Define function to get next position based on action=====================================

def get_next_position(position, action):
    x, y = position
    dx, dy = actions[action]
    next_position = (x + dx, y + dy)
    return next_position if is_valid_position(maze, next_position) else position







#=====================================================Define reward function===================================================
def get_reward(position):
    if maze[position] == 1:
        return 10                     # Goal reward
    elif maze[position] == -1:
        return -1                     # Penalty for hitting an obstacle
    else:
        return -0.1                   # Small penalty for each step taken








#======================================================Q-learning algorithm=================================================================

for episode in range(num_episodes):
    position = (0, 0)  # Start position
    done = False


    
    while not done:
        #====================Choose action based on epsilon-greedy policy=============================
        
        if random.uniform(0, 1) < epsilon:
            action = random.randint(0, 3)                                    # Explore: random action
        else:
            action = np.argmax(q_table[position[0], position[1]])            # Exploit: best action





        
        #=======================================Take action and observe next state and reward====================================
        
        next_position = get_next_position(position, action)
        reward = get_reward(next_position)





        
        #========================================Update Q-value using the Bellman equation========================================
        
        best_next_action = np.argmax(q_table[next_position[0], next_position[1]])
        
        td_target = reward + gamma * q_table[next_position[0], next_position[1], best_next_action]
        
        td_error = td_target - q_table[position[0], position[1], action]
        
        q_table[position[0], position[1], action] += alpha * td_error





        
        #==============================================Update position===============================================
        position = next_position




        
        #===========================================Check if goal is reached============================================
        if maze[position] == 1:
            done = True





    
    #=======================================================Decay epsilon to reduce exploration over time================================
    epsilon *= epsilon_decay









print("Training completed!")
print("Final Q-table:")
print(q_table)



% Training completed!
% Final Q-table:
% [[[ 3.97013875  3.97416476  4.01495051  4.845851  ]
%   [ 4.7378907   4.81161701  4.12058648  5.49539   ]
%   [ 5.4054968   6.2171      4.42682984  5.43323673]
%   [ 0.          0.          0.          0.        ]
%   [-0.0473428   0.08843106 -0.03940399 -0.042661  ]]

%  [[ 0.36199443  5.15332982  1.26723612  0.05544543]
%   [ 0.          0.          0.          0.        ]
%   [ 5.44624324  7.019       5.96735728  6.10584184]
%   [ 0.          0.          0.          0.        ]
%   [-0.0527741   3.61845702  0.14230459  0.10177445]]

%  [[ 0.80828579  1.84919549  1.98371271  6.14400125]
%   [ 4.51401645  1.65500804  2.10403282  7.01715326]
%   [ 6.1772837   6.98873549  6.07146498  7.91      ]
%   [ 7.88335439  8.9         6.90454789  6.67875034]
%   [ 1.13576689  2.69796765  7.84673033  2.99882748]]

%  [[ 0.          0.          0.          0.        ]
%   [ 1.59325789  3.40098732  0.71131279  0.28977213]
%   [ 0.          0.          0.          0.        ]
%   [ 7.84601487 10.          8.75442916  8.80519713]
%   [ 0.          0.          0.          0.        ]]

%  [[-0.03258193 -0.02663555  0.06833728  1.46332571]
%   [ 0.47825841  2.09739054  0.18177722  6.0093313 ]
%   [ 2.78653188  1.40430184  0.87915386  8.90581011]
%   [ 0.          0.          0.          0.        ]
%   [ 0.          0.          0.          0.        ]]]

