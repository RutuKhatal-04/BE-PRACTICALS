
import numpy as np
import random

#==================================Define the maze layout===========================================
maze = np.array([
    [0, 0, 0, -1, 0],
    [0, -1, 0, -1, 0],
    [0, 0, 0, 0, 0],
    [-1, 0, -1, 0, -1],
    [0, 0, 0, 1, 0]
])




#=================================Define hyperparameters=============================================
alpha = 0.1               # Learning rate
gamma = 0.9               # Discount factor
epsilon = 0.8             # Exploration rate
epsilon_decay = 0.995
num_episodes = 500





#==============================Initialize Q-table with zeros (size of the maze x number of actions)=========================================

q_table = np.zeros((maze.shape[0], maze.shape[1], 4)) # 4 possible actions (up, down, left, right)






#===========================================Define actions=======================================================================
actions = {
    0: (-1, 0),  # Up
    1: (1, 0),   # Down
    2: (0, -1),  # Left
    3: (0, 1)    # Right
}





#========================================Define function to check if a position is valid========================================

def is_valid_position(maze, position):
    x, y = position
    return 0 <= x < maze.shape[0] and 0 <= y < maze.shape[1] and maze[x, y] != -1





#=======================================Define function to get next position based on action=====================================

def get_next_position(position, action):
    x, y = position
    dx, dy = actions[action]
    next_position = (x + dx, y + dy)
    return next_position if is_valid_position(maze, next_position) else position







#=====================================================Define reward function===================================================
def get_reward(position):
    if maze[position] == 1:
        return 10                     # Goal reward
    elif maze[position] == -1:
        return -1                     # Penalty for hitting an obstacle
    else:
        return -0.1                   # Small penalty for each step taken








#======================================================Q-learning algorithm=================================================================

for episode in range(num_episodes):
    position = (0, 0)  # Start position
    done = False


    
    while not done:
        #====================Choose action based on epsilon-greedy policy=============================
        
        if random.uniform(0, 1) < epsilon:
            action = random.randint(0, 3)                                    # Explore: random action
        else:
            action = np.argmax(q_table[position[0], position[1]])            # Exploit: best action





        
        #=======================================Take action and observe next state and reward====================================
        
        next_position = get_next_position(position, action)
        reward = get_reward(next_position)





        
        #========================================Update Q-value using the Bellman equation========================================
        
        best_next_action = np.argmax(q_table[next_position[0], next_position[1]])
        
        td_target = reward + gamma * q_table[next_position[0], next_position[1], best_next_action]
        
        td_error = td_target - q_table[position[0], position[1], action]
        
        q_table[position[0], position[1], action] += alpha * td_error





        
        #==============================================Update position===============================================
        position = next_position




        
        #===========================================Check if goal is reached============================================
        if maze[position] == 1:
            done = True





    
    #=======================================================Decay epsilon to reduce exploration over time================================
    epsilon *= epsilon_decay









print("Training completed!")
print("Final Q-table:")
print(q_table)
