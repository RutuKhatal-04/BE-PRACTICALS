{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph structure: {'https://example.com': set(), 'https://google.com/page1': {'https://google.com/page1//www.google.com/'}, 'https://gmail.com/page2': {'https://gmail.com/page2//www.google.com/'}}\n",
      "PageRank scores: {'https://example.com': 0.05000000000000001, 'https://google.com/page1': 0.05000000000000001, 'https://gmail.com/page2': 0.05000000000000001}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "# PageRank Explanation:\n",
    "# PageRank is an algorithm developed by Google to rank web pages based on their importance.\n",
    "# It assigns a score to each webpage, indicating its importance based on the number and quality of links pointing to it.\n",
    "# The core idea is that a page is more important if it is linked to by other important pages.\n",
    "\n",
    "# Uses of PageRank:\n",
    "# PageRank is commonly used in search engines to rank search results, providing relevant and authoritative results.\n",
    "# It is also used in social network analysis, ranking academic papers, and identifying influential nodes in a network.\n",
    "\n",
    "# Why BeautifulSoup:\n",
    "# BeautifulSoup is a Python library used for web scraping, making it easy to extract information from HTML and XML files.\n",
    "# It parses HTML content, making it simple to navigate, search, and modify the parsed HTML tree.\n",
    "# It is chosen here because it is easy to use and efficient for scraping simple HTML content.\n",
    "# Other libraries like Selenium are used for more complex, dynamic content (e.g., JavaScript-based content),\n",
    "# but BeautifulSoup is sufficient for scraping basic internal links from static HTML pages.\n",
    "\n",
    "def get_links(url):\n",
    "    \"\"\"\n",
    "    Scrape internal links from the given URL using BeautifulSoup.\n",
    "    \n",
    "    Arguments:\n",
    "    url (str): The URL to scrape links from.\n",
    "    \n",
    "    Returns:\n",
    "    set: A set of internal links found on the page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Initialize an empty set to store internal links\n",
    "        links = set()\n",
    "        # Loop through all <a> tags with 'href' attributes\n",
    "        for a_tag in soup.find_all(\"a\", href=True):\n",
    "            href = a_tag['href']\n",
    "            # Consider only relative URLs (internal links)\n",
    "            if href.startswith(\"/\"):\n",
    "                # Convert relative link to full URL and add it to the set\n",
    "                links.add(url + href)\n",
    "        \n",
    "        return links\n",
    "    except Exception as e:\n",
    "        # Print an error message if scraping fails\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return set()\n",
    "\n",
    "def build_graph(urls):\n",
    "    \"\"\"\n",
    "    Create a graph dictionary where each URL points to its list of linked URLs.\n",
    "    \n",
    "    Arguments:\n",
    "    urls (list): A list of URLs to include in the graph.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary representing the graph (URL -> set of linked URLs).\n",
    "    \"\"\"\n",
    "    graph = {}\n",
    "    # Iterate over each URL in the list\n",
    "    for url in urls:\n",
    "        # Get the links found on the current URL and add them to the graph\n",
    "        graph[url] = get_links(url)\n",
    "    return graph\n",
    "\n",
    "def page_rank(graph, d=0.85, iterations=50):\n",
    "    \"\"\"\n",
    "    Simple PageRank algorithm to calculate rank scores for each URL in the graph.\n",
    "    \n",
    "    Arguments:\n",
    "    graph (dict): The graph dictionary where each URL points to its linked URLs.\n",
    "    d (float): The damping factor (default is 0.85). It represents the probability that a user continues clicking on links.\n",
    "    iterations (int): The number of iterations to run the algorithm (default is 50).\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary with URLs as keys and their PageRank scores as values.\n",
    "    \"\"\"\n",
    "    n = len(graph)  # Total number of URLs (nodes) in the graph\n",
    "    # Initialize the rank of each URL to 1/n (equal distribution)\n",
    "    ranks = {url: 1 / n for url in graph}\n",
    "\n",
    "    # Iterate multiple times to update the ranks based on link structure\n",
    "    for _ in range(iterations):\n",
    "        new_ranks = {}  # Dictionary to store updated ranks\n",
    "        for url in graph:\n",
    "            # Calculate the new rank score for the current URL\n",
    "            # Sum the contributions from all pages linking to this URL\n",
    "            rank_sum = sum(ranks[link] / len(graph[link]) for link in graph if url in graph[link])\n",
    "            # Update the rank using the damping factor formula\n",
    "            new_ranks[url] = (1 - d) / n + d * rank_sum\n",
    "        \n",
    "        # Update the ranks dictionary with new values for the next iteration\n",
    "        ranks = new_ranks\n",
    "    \n",
    "    return ranks\n",
    "\n",
    "# Example usage of the functions defined above\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a small list of example URLs from the same domain\n",
    "    urls = [\n",
    "        \"https://example.com\",\n",
    "        \"https://google.com/page1\",\n",
    "        \"https://gmail.com/page2\",\n",
    "    ]\n",
    "\n",
    "    # Build the graph from the given URLs\n",
    "    graph = build_graph(urls)\n",
    "    print(\"Graph structure:\", graph)  # Print the structure of the graph (for debugging)\n",
    "\n",
    "    # Calculate PageRank scores for the URLs in the graph\n",
    "    scores = page_rank(graph)\n",
    "\n",
    "    # Display the PageRank scores for each URL\n",
    "    print(\"PageRank scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "     ---------------------------------------- 0.0/147.9 kB ? eta -:--:--\n",
      "     ---------- ---------------------------- 41.0/147.9 kB 1.9 MB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/147.9 kB 656.4 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 102.4/147.9 kB 737.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ 147.9/147.9 kB 882.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\khata\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.28.2)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\khata\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\khata\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\khata\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\khata\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests) (1.26.15)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\khata\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
